Dharma

---

make sure i build things for the right byte order (big/little endian)

---

sytstem architecture
- would be nice if there didnt need to be threasholds and static values. keep to a minimun.
- idk if we will need to do this, but u create a cluster and load balance to the nodes. what if
we created multiple clusters, load balanced btw the clusters, which load balance to the nodes,
so that the equation can balance out. so each cluster would have a slightly different config,
and each cluster communicates to one another.

there is unpredicatable traffic, and predictable traffic.
- predictable traffic are cron tasks. when u fire off a cron task, have there be a way to
tell dharma that this task will be run again in the future so dharma can configure itself in anticipation (might be a hack)

HA (high availability)
- 2 load balancers (active, and passive in case of failure)
  - https://www.digitalocean.com/community/tutorials/how-to-use-floating-ips-on-digitalocean
  - https://www.digitalocean.com/community/tutorials/how-to-create-a-high-availability-setup-with-heartbeat-and-floating-ips-on-ubuntu-14-04
  - when the passive LB becomes active, we spin up another passive and place it in queue.
- could assign odd ips to workers and even ips to routers. have a system for finding people to communicate w/.
- we should extend this concept, and have failure backup servers ready for every server type. if a router goes down, we should be able to spin one back up in like 10 seconds.

when u update dependencies, we just clone the cluster and add new files. we use a remote1 config,
so that means different (mirror) databases, and we load balanace requests over to this enviornment
but these are just copys of requeests, the original request still gets handled by the remote0 cluster.
we make sure the results are identical for however long (@todo)
- need a way to not fuck shit up, like inserting duplicate files and db entries.

overrides
- special config that overides some default behavior intented for safty. e.g. response time target ceiling.

configurations
- reconfigure (see if it needs to scale) every: 60 seconds
- response time target: // deploy, test, load test, take the average response time data, and use it as ur beginning target
- cost data: max spending. // if exceeded, we stop scaling, and email whoever is on the list


EVERY SERVER KNOWS EVERYTHING. anybody can be everybody.

@todo there is some recursive handshake that happens when agreeing on scaling.
@todo delegating worker tasks out among workers
@todo how to handle updates.
- config updates?
- dependency updates?
- resource updates?
- static asset updates?


All handlers are timed, and they all share their handler times w/ their requesters.
if you microtime a http endpoint, part of the request time will include the time workers and
other apis spend doing work. so all system endpoints microtime themselves, and when they are called
upon from another part of the system, they appned their handler time in the reponse. so we know, exactly
how long each component of the system spend doing work.
- OR -
BETTER
EVERYONE IS RESPONSIBLE FOR THEMSELVES



- monitor http response time, have a default target (@todo just test a real app).
  - if an unreasonably low target is given, one that cant be attained, we scale up until diminishing
  returns are met (@todo quantify).
  - if an unreasonably high target is given, there is a brahma threashold to prevent u from being a funcking dunce. (@todo are threasholds a hack)
  - what is the workflow like?
    - how often do u review data. what data...each handled request is decorated and microtimed. we check
    to see the average response time for the previous X seconds: (@todo when do u check and why)
      - if we are to slow, we add nodes:
        - @todo how many?
        - is there a grace window? no, servers are cheap.
        - @todo what do u do on failure?

---

system expectations.
- there will be http requests from various clients (laptops, phones)
rendering text, images, streaming audio and video. Expected response time (> X).
  - these requests are routed to a node, of type 'router'...

start w/ a router and an worker
- why
  - it is assumed that there will be work tasks, and work tasks block the thread,
  and we cant have that, so if 1 doesn't work, we see if 2 works, and it does.

explain how the system works.

if a worker gets busy
- what happens

if a router gets busy
- what happens

---

Spec

1 active LB and 1 passive LB deployed on d.o (*1)
we deploy 2 nodes for redundancy (*2)

1 request comes in, and based on the URL, we know the appropriate node route handler (*3)
forward request to first node in set (*4)
node handles the request, and replys to LB, which then replys to the client. (*5)
LB calculates request duration and adds it to the route map. {endpointName: '', route: '', perfAvg: '', durations: []}

another request comes in (w/o any open requests) - which server to choose and when to scale up/down?
- nodes {nodeIP, wait: adds up request duration} chooses the node w/ the smallest wait. (*6, *7)
how to segregate long infrequent tasks to a subset of nodes? need to track endpoint frequency.

Appendix
*1 HA setup. each node pings the active LB every 5 (configurable, arbitrary) seconds, and on error, it activates the backup LB.
*2 each node route defines a test. on deploy, we test each route and gather a perf benchmark to be used by LB.
*3 route parsing done in C. create a map, {endpointName: '', route: ''} and forward the request
to a node server. C also forces https, prevents DDoS, ratelimits, letsencrypt, and forwards asset requests to s3 w/ static caching.
*4 c keeps IPs of all available nodes. need to hot reload when we add/remove nodes.
*5 https://softwareengineering.stackexchange.com/questions/312956/what-does-a-load-balancer-return
*6 all incoming requests increment the nodes {wait: ''} and finished requests decrement.
*7 finished requests compare the perfAvg to actual duration and decide to add/remove nodes.
when to add more nodes? lets not be arbitrary. i think you can set config value which for waitMultiplier.
so if an endpoint is 10ms perfAvg, w/ a waitMultiplier of 2x, u dont scale until that endpoint's actual
duration (find median across all nodes) is > 20ms. this is how u regulate cost.

delegation. an equation takeIng:
- droplet spinup time
- perfAvg (relative to other endpoints),
- actual perf (realtive to other endpoints)
- endpoint call frequency
- #nodes,
- cron tasks

say a endpoint is called (perf === 50ms)
LB to a medium node. nodes can be small, medium, large, or ...

3 4 4 5 5 6 6 7 9
10 17 20 20 20
34
50 50
63
200

function waitTime(interval = 10000ms, frequency = 1 req/ms, time per request = 10ms) {
  return (requestDuration * interval) - (frequency * interval); // 9000 ms after a 1 minute interval
}

// over the next 15 seconds, 30 seconds, ...., highest perfAvg.
function predictTraffic() {

};

function groupWaitTime([3,4,4,5,5,6,6,7,9]) {
  return obj.reduce((p, c) => {
    return p+c;
  }, 0);
};

0-9   groupWaitTime(predictTraffic(obj, 1000*15))
10-19 groupWaitTime(predictTraffic(obj, 1000*15))
20-29 groupWaitTime(predictTraffic(obj, 1000*15))

[1,2,...29,30]
function optimizeGroups(groups) {
  var matrix = possibilities(groups);
  var waits = matrix.map((row, i) => {
    return {key, time: groupWaitTime(row), numNodes: getNumNodes(groupWaitTime(row), waitMultiplier)}
  })
  waits = orderAsc(waits);
  var fastestConfig = waits.find(wait => {
    if (perfAvg * waitMultiplier < wait) return wait;
  });
  return fastestConfig;
}

function getNumNodes(time, waitMultiplier, averagePerf) {

};


Notes
If there is the need to tie a client to a particular application server — in other words,
make the client’s session “sticky” or “persistent” in terms of always trying to select a
particular server — the ip-hash load balancing mechanism can be used.
- how do cookies work?
- how does realtime work?
C has hot reloading of config.
if the LB communicates using http can we be sure there are no man in the middle attacks?
maybe set a DDoS limit, which can be changed during perf testing.
How to handle cron? we may need to simulate cron jobs by first capturing normal traffic
and applying the cron load over time.
for priming and testing we need a way to spin up a test DB on mongo and redis.

---

long todos
- how to integrate a realtime server that maintains persistent connections w/ clients over time.
