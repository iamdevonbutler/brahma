Dharma

---

sytstem architecture
- would be nice if there didnt need to be threasholds and static values. keep to a minimun.

HA (high availability)
- 2 load balancers (active, and passive in case of failure)
  - https://www.digitalocean.com/community/tutorials/how-to-use-floating-ips-on-digitalocean
  - https://www.digitalocean.com/community/tutorials/how-to-create-a-high-availability-setup-with-heartbeat-and-floating-ips-on-ubuntu-14-04
  - when the passive LB becomes active, we spin up another passive and place it in queue.
- could assign odd ips to workers and even ips to routers. have a system for finding people to communicate w/.
- we should extend this concept, and have failure backup servers ready for every server type. if a router goes down, we should be able to spin one back up in like 10 seconds.

when u update dependencies, we just clone the cluster and add new files. we use a remote1 config,
so that means different (mirror) databases, and we load balanace requests over to this enviornment
but these are just copys of requeests, the original request still gets handled by the remote0 cluster.
we make sure the results are identical for however long (@todo)
- need a way to not fuck shit up, like inserting duplicate files and db entries.

overrides
- special config that overides some default behavior intented for safty. e.g. response time target ceiling.

configurations
- reconfigure (see if it needs to scale) every: 60 seconds
- response time target: // deploy, test, load test, take the average response time data, and use it as ur beginning target
- cost data: max spending. // if exceeded, we stop scaling, and email whoever is on the list


EVERY SERVER KNOWS EVERYTHING. anybody can be everybody.

@todo there is some recursive handshake that happens when agreeing on scaling.
@todo delegating worker tasks out among workers
@todo how to handle updates.
- config updates?
- dependency updates?
- resource updates?
- static asset updates?


All handlers are timed, and they all share their handler times w/ their requesters.
if you microtime a http endpoint, part of the request time will include the time workers and
other apis spend doing work. so all system endpoints microtime themselves, and when they are called
upon from another part of the system, they appned their handler time in the reponse. so we know, exactly
how long each component of the system spend doing work.
- OR -
BETTER
EVERYONE IS RESPONSIBLE FOR THEMSELVES



- monitor http response time, have a default target (@todo just test a real app).
  - if an unreasonably low target is given, one that cant be attained, we scale up until diminishing
  returns are met (@todo quantify).
  - if an unreasonably high target is given, there is a brahma threashold to prevent u from being a funcking dunce. (@todo are threasholds a hack)
  - what is the workflow like?
    - how often do u review data. what data...each handled request is decorated and microtimed. we check
    to see the average response time for the previous X seconds: (@todo when do u check and why)
      - if we are to slow, we add nodes:
        - @todo how many?
        - is there a grace window? no, servers are cheap.
        - @todo what do u do on failure?

---

system expectations.
- there will be http requests from various clients (laptops, phones)
rendering text, images, streaming audio and video. Expected response time (> X).
  - these requests are routed to a node, of type 'router'...

start w/ a router and an worker
- why
  - it is assumed that there will be work tasks, and work tasks block the thread,
  and we cant have that, so if 1 doesn't work, we see if 2 works, and it does.

explain how the system works.

if a worker gets busy
- what happens

if a router gets busy
- what happens

---

long todos
- how to integrate a realtime server that maintains persistent connections w/ clients over time.
